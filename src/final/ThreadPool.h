#pragma once

#include <vector>
#include <queue>
#include <thread>
#include <mutex>
#include <condition_variable>
#include <functional>
#include <future>
#include <memory>
#include <atomic>
#include <chrono>
#include <stdexcept>

/**
 * ğŸ¯ ç”Ÿäº§çº§çº¿ç¨‹æ±  - æœ€ç»ˆç‰ˆæœ¬
 * 
 * ç‰¹æ€§ï¼š
 * âœ… æ”¯æŒä»»åŠ¡è¿”å›å€¼ (std::future)
 * âœ… ä»»åŠ¡ä¼˜å…ˆçº§
 * âœ… æ€§èƒ½ç»Ÿè®¡
 * âœ… å¼‚å¸¸å®‰å…¨
 * âœ… ä¼˜é›…å…³é—­
 * âœ… çº¿ç¨‹å®‰å…¨
 * âœ… æ˜“äºä½¿ç”¨çš„ API
 */

namespace thread_pool {

enum class Priority {
    LOW = 0,
    NORMAL = 1,
    HIGH = 2,
    CRITICAL = 3
};

class ThreadPool {
public:
    /**
     * æ„é€ å‡½æ•°
     * @param num_threads çº¿ç¨‹æ•°é‡ï¼Œé»˜è®¤ä¸ºç¡¬ä»¶å¹¶å‘æ•°
     * @param enable_stats æ˜¯å¦å¯ç”¨ç»Ÿè®¡åŠŸèƒ½ï¼Œé»˜è®¤å¯ç”¨
     */
    explicit ThreadPool(
        size_t num_threads = std::thread::hardware_concurrency(),
        bool enable_stats = true)
        : stop_(false)
        , enable_stats_(enable_stats)
        , total_tasks_(0)
        , completed_tasks_(0)
        , failed_tasks_(0)
        , total_execution_time_(0) {
        
        if (num_threads == 0) {
            throw std::invalid_argument("çº¿ç¨‹æ•°é‡å¿…é¡»å¤§äº0");
        }
        
        creation_time_ = std::chrono::steady_clock::now();
        
        for (size_t i = 0; i < num_threads; ++i) {
            workers_.emplace_back([this]() {
                worker_thread();
            });
        }
    }
    
    /**
     * ææ„å‡½æ•° - è‡ªåŠ¨è°ƒç”¨ shutdown()
     */
    ~ThreadPool() {
        shutdown();
    }
    
    // ç¦æ­¢æ‹·è´å’Œç§»åŠ¨
    ThreadPool(const ThreadPool&) = delete;
    ThreadPool& operator=(const ThreadPool&) = delete;
    ThreadPool(ThreadPool&&) = delete;
    ThreadPool& operator=(ThreadPool&&) = delete;
    
    /**
     * æäº¤ä»»åŠ¡ï¼ˆå¸¦ä¼˜å…ˆçº§ï¼‰
     * 
     * @tparam F å‡½æ•°ç±»å‹
     * @tparam Args å‚æ•°ç±»å‹
     * @param priority ä»»åŠ¡ä¼˜å…ˆçº§
     * @param f è¦æ‰§è¡Œçš„å‡½æ•°
     * @param args å‡½æ•°å‚æ•°
     * @return std::future<è¿”å›å€¼ç±»å‹>
     * 
     * ç¤ºä¾‹:
     *   auto result = pool.submit(Priority::HIGH, [](int x) { return x * 2; }, 21);
     *   std::cout << result.get() << std::endl;  // è¾“å‡º 42
     */
    template<typename F, typename... Args>
    auto submit(Priority priority, F&& f, Args&&... args) 
        -> std::future<typename std::result_of<F(Args...)>::type> {
        
        using return_type = typename std::result_of<F(Args...)>::type;
        
        auto task = std::make_shared<std::packaged_task<return_type()>>(
            std::bind(std::forward<F>(f), std::forward<Args>(args)...)
        );
        
        std::future<return_type> result = task->get_future();
        
        {
            std::unique_lock<std::mutex> lock(queue_mutex_);
            
            if (stop_) {
                throw std::runtime_error("Cannot submit task: ThreadPool is stopped");
            }
            
            tasks_.emplace(priority, [this, task]() {
                execute_task([task]() { (*task)(); });
            });
            
            if (enable_stats_) {
                total_tasks_++;
            }
        }
        
        condition_.notify_one();
        return result;
    }
    
    /**
     * æäº¤ä»»åŠ¡ï¼ˆé»˜è®¤ä¼˜å…ˆçº§ï¼‰
     */
    template<typename F, typename... Args>
    auto submit(F&& f, Args&&... args) 
        -> std::future<typename std::result_of<F(Args...)>::type> {
        return submit(Priority::NORMAL, std::forward<F>(f), std::forward<Args>(args)...);
    }
    
    /**
     * ä¼˜é›…å…³é—­ï¼šç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
     */
    void shutdown() {
        {
            std::unique_lock<std::mutex> lock(queue_mutex_);
            if (stop_) return;  // å·²ç»å…³é—­
            stop_ = true;
        }
        
        condition_.notify_all();
        
        for (auto& worker : workers_) {
            if (worker.joinable()) {
                worker.join();
            }
        }
    }
    
    /**
     * å¼ºåˆ¶å…³é—­ï¼šä¸¢å¼ƒæœªæ‰§è¡Œçš„ä»»åŠ¡
     */
    void shutdown_now() {
        {
            std::unique_lock<std::mutex> lock(queue_mutex_);
            if (stop_) return;
            
            // æ¸…ç©ºä»»åŠ¡é˜Ÿåˆ—
            while (!tasks_.empty()) {
                tasks_.pop();
            }
            
            stop_ = true;
        }
        
        condition_.notify_all();
        
        for (auto& worker : workers_) {
            if (worker.joinable()) {
                worker.join();
            }
        }
    }
    
    /**
     * æŸ¥è¯¢æ¥å£
     */
    size_t thread_count() const { return workers_.size(); }
    size_t pending_tasks() const {
        std::unique_lock<std::mutex> lock(queue_mutex_);
        return tasks_.size();
    }
    size_t total_tasks() const { return total_tasks_.load(); }
    size_t completed_tasks() const { return completed_tasks_.load(); }
    size_t failed_tasks() const { return failed_tasks_.load(); }
    bool is_stopped() const {
        std::unique_lock<std::mutex> lock(queue_mutex_);
        return stop_;
    }
    
    /**
     * è·å–å¹³å‡ä»»åŠ¡æ‰§è¡Œæ—¶é—´ï¼ˆå¾®ç§’ï¼‰
     */
    double average_execution_time() const {
        if (!enable_stats_ || completed_tasks_ == 0) {
            return 0.0;
        }
        return static_cast<double>(total_execution_time_) / completed_tasks_;
    }
    
    /**
     * è·å–è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
     */
    long long uptime() const {
        auto now = std::chrono::steady_clock::now();
        return std::chrono::duration_cast<std::chrono::seconds>(
            now - creation_time_).count();
    }

private:
    /**
     * ä¼˜å…ˆçº§ä»»åŠ¡åŒ…è£…
     */
    struct PriorityTask {
        Priority priority;
        std::function<void()> func;
        
        PriorityTask(Priority p = Priority::NORMAL, std::function<void()> f = nullptr)
            : priority(p), func(std::move(f)) {}
        
        bool operator<(const PriorityTask& other) const {
            return priority < other.priority;
        }
    };
    
    /**
     * å·¥ä½œçº¿ç¨‹å‡½æ•°
     */
    void worker_thread() {
        while (true) {
            PriorityTask task;
            
            {
                std::unique_lock<std::mutex> lock(queue_mutex_);
                condition_.wait(lock, [this]() {
                    return stop_ || !tasks_.empty();
                });
                
                if (stop_ && tasks_.empty()) {
                    return;
                }
                
                task = std::move(const_cast<PriorityTask&>(tasks_.top()));
                tasks_.pop();
            }
            
            task.func();
        }
    }
    
    /**
     * æ‰§è¡Œä»»åŠ¡å¹¶ç»Ÿè®¡
     */
    void execute_task(std::function<void()> task) {
        auto start = enable_stats_ ? std::chrono::steady_clock::now() 
                                    : std::chrono::steady_clock::time_point{};
        
        try {
            task();
            if (enable_stats_) {
                completed_tasks_++;
            }
        } catch (...) {
            if (enable_stats_) {
                failed_tasks_++;
            }
            // é™é»˜å¤±è´¥ï¼Œä¸æŠ›å‡ºå¼‚å¸¸
        }
        
        if (enable_stats_) {
            auto end = std::chrono::steady_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::microseconds>(
                end - start).count();
            total_execution_time_ += duration;
        }
    }
    
    std::vector<std::thread> workers_;
    std::priority_queue<PriorityTask> tasks_;
    
    mutable std::mutex queue_mutex_;
    std::condition_variable condition_;
    bool stop_;
    
    // ç»Ÿè®¡
    bool enable_stats_;
    std::atomic<size_t> total_tasks_;
    std::atomic<size_t> completed_tasks_;
    std::atomic<size_t> failed_tasks_;
    std::atomic<long long> total_execution_time_;
    std::chrono::steady_clock::time_point creation_time_;
};

} // namespace thread_pool
